::tools
[[understanding-AI-->understanding-AI]]

::strategies


::ELK


::tammy-alignment


::transformative-ai


::deceptive-alignment
blaise pascal
he doesnt really care about the bible, but he just wants to avoid hell. so hell do what it says and appear christian.



[[inner-alignment-->inner-alignment]
[[outer-alignment-->outer-alignment]
[[mesa-optimization-->mesa-optimization]]

https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment
slides:
https://docs.google.com/presentation/d/1IzmmUSvhjeGhc_nc8Wd7-hB9_rSeES8JvEvKzQ8uHBI/edit

go to the deceptive-alignment map for more info.

::what-is-AI-thinking


::mesa-optimization
" a mesa-objective, which is just something that itâ€™s trying to optimize for"

::outer-alignment


::value-learning


::alignment-minetest
in pins: https://docs.google.com/document/d/1Oz8neKkXYaWzmRg9ZeYqlFIgYENUkHLhcviDG2hQRiE/edit?usp=sharing

"The goal of this project is to provide a rich and easily moddable environment that alignment researchers can use to test many aspects of alignment and alignment techniques."

::aesthetic-models


::accelerating-alignment
In terms of concrete tools, we're thinking about things like:
(HITL = human in the loop)
- HITL interface to explore LM generations about alignment work
- HITL interface to help alignment researchers write more quickly (e.g. expand bullet points, rewrite text, etc.)
- Tools to automatically generate alignment forum content like posts/comments from particular users and on particular topics (possibly integrated with LW/AF via the website, and extension, or a mirror)
- A mirror LW/AF website filled with autogenerated content (like https://reddit.com/r/SubSimulatorGPT2/)

::agent-foundations
in channel description:
Decision Theory, Embedded Agency, Corrigibility, etc. Introduction (Warning: Math): 
https://www.alignmentforum.org/s/Rm6oQRJJmhGCcLvxh

::prosaic-alignment


::EAI-channels
[[prosaic-alignment-->prosaic-alignment]]
[[agent-foundations-->agent-foundations]]
[[accelerating-alignment-->accelerating-alignment]]
[[aesthetic-models-->aesthetic-models]]
[[alignment-minetest-->alignment-minetest]]

::oracle-AI


::general-alignment


::to-read


::decision-theory


::too-niche-or-too-hard


::failing-strategies


::AIXI


::teaching-AI-values


::complexity-of-value


::embedded-agency


::agency-theory


::RLHF


::interpretability


::understanding-AI
[[ELK-->ELK]]

::goodharts-law


::reward-functions
